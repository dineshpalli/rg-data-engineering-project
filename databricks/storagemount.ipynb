{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a10fac1-e127-4317-a2f7-ab96a74dbdb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.databricks.passthrough.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee38904-419a-4a51-a257-40f328d2451a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a configuration dictionary for mounting Azure Data Lake Storage Gen2 (ADLS Gen2).\n",
    "# These configurations are necessary to authenticate and connect to ADLS Gen2\n",
    "# using Azure Active Directory (Azure AD) credential passthrough.\n",
    "configs = {\n",
    "  # Specifies the authentication type for accessing Azure Data Lake Storage.\n",
    "  # \"CustomAccessToken\" indicates that a custom token provider will be used.\n",
    "  # For credential passthrough, Databricks uses a specific mechanism to generate\n",
    "  # and manage these tokens based on the user's Azure AD identity.\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "\n",
    "  # Specifies the class that provides the custom access token for ADLS Gen2.\n",
    "  # spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "  # dynamically retrieves the name of the token provider class that is configured\n",
    "  # at the cluster level for ADLS Gen2 credential passthrough.\n",
    "  # This ensures that the mount uses the correct, cluster-configured mechanism\n",
    "  # for secure token generation, enhancing security and maintainability.\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "}\n",
    "\n",
    "# --- Unmount existing mount points if they exist ---\n",
    "# This section ensures that the script can be run multiple times without failing due to\n",
    "# pre-existing mount points. Attempting to mount to a path that is already a mount point\n",
    "# will result in an error. Unmounting first provides a clean state.\n",
    "\n",
    "# Check if a mount point already exists at \"/mnt/bronze\".\n",
    "# dbutils.fs.mounts() returns a list of all active mount points in the Databricks File System (DBFS).\n",
    "# The list comprehension iterates through these mounts and checks if any `mountPoint` attribute\n",
    "# matches \"/mnt/bronze\".\n",
    "if any(mount.mountPoint == \"/mnt/bronze\" for mount in dbutils.fs.mounts()):\n",
    "  # If the mount point \"/mnt/bronze\" exists, unmount it.\n",
    "  # dbutils.fs.unmount() removes the specified mount point from DBFS.\n",
    "  # This is crucial for idempotency, allowing the script to be re-run reliably.\n",
    "  dbutils.fs.unmount(\"/mnt/bronze\")\n",
    "\n",
    "# Check if a mount point already exists at \"/mnt/silver\".\n",
    "# Similar to the \"/mnt/bronze\" check, this verifies if \"/mnt/silver\" is already mounted.\n",
    "if any(mount.mountPoint == \"/mnt/silver\" for mount in dbutils.fs.mounts()):\n",
    "  # If the mount point \"/mnt/silver\" exists, unmount it.\n",
    "  # This prevents errors if the script is run again after a successful previous execution\n",
    "  # or if the mount point was created manually.\n",
    "  dbutils.fs.unmount(\"/mnt/silver\")\n",
    "\n",
    "# Check if a mount point already exists at \"/mnt/gold\".\n",
    "# This performs the same check for the \"/mnt/gold\" mount point.\n",
    "if any(mount.mountPoint == \"/mnt/gold\" for mount in dbutils.fs.mounts()):\n",
    "  # If the mount point \"/mnt/gold\" exists, unmount it.\n",
    "  # This ensures a clean setup for the subsequent mount operation.\n",
    "  dbutils.fs.unmount(\"/mnt/gold\")\n",
    "\n",
    "# --- Mount the ADLS Gen2 containers to DBFS ---\n",
    "# Mounting makes ADLS Gen2 storage accessible as if it were part of the local file system\n",
    "# within Databricks (DBFS). This simplifies file access paths in notebooks and jobs.\n",
    "# The `extra_configs = configs` part enables credential passthrough, meaning that\n",
    "# access to the data lake will be governed by the Azure AD identity of the user\n",
    "# running the notebook or job, enhancing security.\n",
    "\n",
    "# Mount the 'bronze' container from ADLS Gen2 to \"/mnt/bronze\" in DBFS.\n",
    "# The 'bronze' layer typically stores raw, ingested data.\n",
    "# \"abfss://\" is the scheme for Azure Blob File System, which is used for ADLS Gen2.\n",
    "# \"bronze@datalakedeproj.dfs.core.windows.net/\" is the URI for the 'bronze' container\n",
    "# within the 'datalakedeproj' storage account.\n",
    "# \"/mnt/bronze\" is the local path in DBFS where the 'bronze' container's contents will be accessible.\n",
    "# The `extra_configs` argument applies the previously defined authentication settings,\n",
    "# enabling credential passthrough.\n",
    "# The comment \"# Optionally, add <directory-name> to the source URI of your mount point.\"\n",
    "# indicates that you could also mount a specific sub-directory within the container\n",
    "# e.g., \"abfss://bronze@datalakedeproj.dfs.core.windows.net/specific_folder/\".\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://bronze@datalakedeproj.dfs.core.windows.net/\", # Source URI of the ADLS Gen2 container\n",
    "  mount_point = \"/mnt/bronze\",                                  # DBFS path to mount to\n",
    "  extra_configs = configs                                       # Configurations for authentication (credential passthrough)\n",
    ")\n",
    "\n",
    "# Mount the 'silver' container from ADLS Gen2 to \"/mnt/silver\" in DBFS.\n",
    "# The 'silver' layer typically stores data that has undergone some cleaning,\n",
    "# transformation, or enrichment from the 'bronze' layer.\n",
    "# The parameters follow the same pattern as the 'bronze' mount.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://silver@datalakedeproj.dfs.core.windows.net/\", # Source URI of the ADLS Gen2 container\n",
    "  mount_point = \"/mnt/silver\",                                  # DBFS path to mount to\n",
    "  extra_configs = configs                                       # Configurations for authentication\n",
    ")\n",
    "\n",
    "# Mount the 'gold' container from ADLS Gen2 to \"/mnt/gold\" in DBFS.\n",
    "# The 'gold' layer typically stores highly refined, aggregated data ready for\n",
    "# analytics, reporting, or consumption by downstream applications.\n",
    "# The parameters follow the same pattern as the 'bronze' and 'silver' mounts.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://gold@datalakedeproj.dfs.core.windows.net/\",   # Source URI of the ADLS Gen2 container\n",
    "  mount_point = \"/mnt/gold\",                                    # DBFS path to mount to\n",
    "  extra_configs = configs                                       # Configurations for authentication\n",
    ")\n",
    "\n",
    "# After these commands are executed, data within the 'bronze', 'silver', and 'gold' containers\n",
    "# in the 'datalakedeproj' ADLS Gen2 account will be accessible via DBFS paths\n",
    "# \"/mnt/bronze\", \"/mnt/silver\", and \"/mnt/gold\" respectively.\n",
    "# Access permissions will be determined by the Azure AD identity of the user executing commands,\n",
    "# thanks to credential passthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a13af9-dfb9-4ca9-a78e-da199d6f4e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/mnt/bronze/SalesLT/\ndbfs:/mnt/bronze/SalesLT/Address/\ndbfs:/mnt/bronze/SalesLT/Address/Address.parquet\ndbfs:/mnt/bronze/SalesLT/Customer/\ndbfs:/mnt/bronze/SalesLT/Customer/Customer.parquet\ndbfs:/mnt/bronze/SalesLT/CustomerAddress/\ndbfs:/mnt/bronze/SalesLT/CustomerAddress/CustomerAddress.parquet\ndbfs:/mnt/bronze/SalesLT/Product/\ndbfs:/mnt/bronze/SalesLT/Product/Product.parquet\ndbfs:/mnt/bronze/SalesLT/ProductCategory/\ndbfs:/mnt/bronze/SalesLT/ProductCategory/ProductCategory.parquet\ndbfs:/mnt/bronze/SalesLT/ProductDescription/\ndbfs:/mnt/bronze/SalesLT/ProductDescription/ProductDescription.parquet\ndbfs:/mnt/bronze/SalesLT/ProductModel/\ndbfs:/mnt/bronze/SalesLT/ProductModel/ProductModel.parquet\ndbfs:/mnt/bronze/SalesLT/ProductModelProductDescription/\ndbfs:/mnt/bronze/SalesLT/ProductModelProductDescription/ProductModelProductDescription.parquet\ndbfs:/mnt/bronze/SalesLT/SalesOrderDetail/\ndbfs:/mnt/bronze/SalesLT/SalesOrderDetail/SalesOrderDetail.parquet\ndbfs:/mnt/bronze/SalesLT/SalesOrderHeader/\ndbfs:/mnt/bronze/SalesLT/SalesOrderHeader/SalesOrderHeader.parquet\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- List the contents of the mounted ADLS Gen2 containers ---\n",
    "# These commands use the Databricks utility `dbutils.fs.ls()` to list files and directories\n",
    "# within the specified DBFS paths. Since '/mnt/bronze', '/mnt/silver', and '/mnt/gold'\n",
    "# were previously set up as mount points for ADLS Gen2 containers, these commands\n",
    "# effectively list the contents of those respective containers.\n",
    "# This is useful for verifying data, debugging, or understanding the structure of the data lake layers.\n",
    "\n",
    "# List the contents of the '/mnt/bronze' directory.\n",
    "# `dbutils.fs.ls()` is a Databricks File System utility that lists the files and directories\n",
    "# at the given path.\n",
    "# '/mnt/bronze' is the mount point for the 'bronze' ADLS Gen2 container, which typically\n",
    "# stores raw data ingested from source systems.\n",
    "# Why: This command is used to inspect the raw data that has landed in the bronze layer.\n",
    "# It helps to:\n",
    "#   - Verify that data ingestion processes have completed successfully.\n",
    "#   - Check the names, sizes, and modification times of files/directories.\n",
    "#   - Understand the structure of the raw data (e.g., subdirectories for different tables or dates).\n",
    "#   - Debug issues if expected data is missing or has incorrect formats.\n",
    "dbutils.fs.ls('/mnt/bronze')\n",
    "\n",
    "# # Define a function to recursively list all files and directories in a given DBFS path\n",
    "# def recursive_list(path):\n",
    "#     # Use dbutils.fs.ls() to list the contents (files and folders) at the specified path\n",
    "#     files = dbutils.fs.ls(path)\n",
    "    \n",
    "#     # Loop through each item returned by dbutils.fs.ls()\n",
    "#     for f in files:\n",
    "#         # Print the full path of the item (this could be a file or a directory)\n",
    "#         print(f.path)\n",
    "        \n",
    "#         # Check if the current item is a directory\n",
    "#         if f.isDir():\n",
    "#             # If it is a directory, recursively call the function to list its contents\n",
    "#             recursive_list(f.path)\n",
    "\n",
    "# # Call the function on the bronze mount point to begin the recursive listing\n",
    "# # This will print out the structure of all files and subdirectories under /mnt/bronze\n",
    "# recursive_list(\"/mnt/bronze\")\n",
    "\n",
    "# List the contents of the '/mnt/silver' directory.\n",
    "# '/mnt/silver' is the mount point for the 'silver' ADLS Gen2 container. This layer\n",
    "# typically holds data that has been cleaned, transformed, and conformed from the bronze layer.\n",
    "# It might involve data type corrections, filtering, or minor enrichments.\n",
    "# Why: This command is used to examine the processed data in the silver layer.\n",
    "# It helps to:\n",
    "#   - Confirm that the bronze-to-silver transformation jobs have run and produced output.\n",
    "#   - Inspect the structure and format of the cleaned data (e.g., Parquet or Delta files).\n",
    "#   - Verify that expected tables or datasets are present after initial processing.\n",
    "#   - Aid in debugging transformation logic if the output is not as expected.\n",
    "dbutils.fs.ls('/mnt/silver')\n",
    "\n",
    "# List the contents of the '/mnt/gold' directory.\n",
    "# '/mnt/gold' is the mount point for the 'gold' ADLS Gen2 container. This layer\n",
    "# typically stores highly curated, aggregated, and business-ready data, often\n",
    "# organized into data models suitable for analytics, reporting, or direct consumption\n",
    "# by business intelligence tools or applications.\n",
    "# Why: This command is used to view the final, production-ready datasets in the gold layer.\n",
    "# It helps to:\n",
    "#   - Ensure that the silver-to-gold aggregation and final transformation pipelines have completed.\n",
    "#   - Verify the presence and structure of the datasets that will be used for business insights.\n",
    "#   - Check that the data is organized correctly for consumption (e.g., specific tables or views).\n",
    "#   - Provide a quick way to see what data is available for end-users or downstream systems.\n",
    "dbutils.fs.ls('/mnt/gold')\n",
    "\n",
    "# The output of each `dbutils.fs.ls()` command is typically a list of FileInfo objects,\n",
    "# where each object contains details about a file or directory, such as its path, name,\n",
    "# size, and modification timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e79b13-438b-4653-be3f-49f1f12f0f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "storagemount",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}